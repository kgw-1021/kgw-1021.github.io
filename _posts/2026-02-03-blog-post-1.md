---
title: 'EKI-MPPI Research Note'
date: 2026-02-03
permalink: /posts/2026/02/blog-post-1/
tags:
  - control
  - MPPI
  - planning
  - EKI
  - Variational Inference
  - Stein Variation Gradient Descent
---

<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$', '$$'], ['\\[', '\\]']] 
    },
    svg: {
      fontCache: 'global'
    }
  };
</script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
</script>

## 1. Motivation

Model Predictive Path Integral control (MPPI) is a widely used sampling-based optimal control method.
In practice, MPPI computes a weighted average of sampled control trajectories and executes the mean control.

However, in multimodal cost landscapes such as obstacle avoidance, this averaging process can produce
infeasible solutions by interpolating across modes.

Recent studies reinterpret MPPI from a variational inference perspective.
In particular, **Stein Variational MPPI (SV-MPPI)** replaces importance-weighted averaging with
particle-based reverse KL minimization.

In this note, we show that replacing the MPPI update with an **Ensemble Kalman Inversion (EKI)** update
corresponds to solving a **Gaussian-restricted reverse KL minimization problem**.
We derive the update rules and compare them with Stein Variational Gradient Descent (SVGD).

---

## 2. MPPI Posterior as a Control Distribution

Let the control sequence be denoted by $u \in \mathbb{R}^d$.
MPPI implicitly defines an optimal control posterior of the form

$$
p^\star(u) \propto \exp\left(-\frac{1}{\lambda} J(u)\right) p_0(u),
$$

where $J(u)$ is the trajectory cost, $\lambda$ is the temperature parameter,
and $p_0(u) = \mathcal{N}(\mu_0, \Sigma_0)$ is a Gaussian prior.

Vanilla MPPI estimates the posterior expectation

$$
\mathbb{E}_{p^\star}[u]
$$

via importance sampling.
This procedure corresponds to approximating the posterior under a **forward KL divergence**.

---

## 3. Variational Inference and Reverse KL Objective

Instead of estimating expectations, we consider approximating the posterior distribution
by minimizing the reverse KL divergence

$$
\min_q \; \mathrm{KL}(q(u) \,\|\, p^\star(u)).
$$

Reverse KL divergence is mode-seeking:
the approximation $q$ tends to concentrate on a single dominant mode of $p^\star$.

The resulting behavior strongly depends on the choice of variational family for $q(u)$.

---

## 4. Stein Variational MPPI (SV-MPPI)

SV-MPPI represents the variational distribution as an empirical particle distribution

$$
q(u) = \frac{1}{N} \sum_{i=1}^N \delta(u - u^{(i)}).
$$

Stein Variational Gradient Descent (SVGD) performs functional gradient descent
on the reverse KL objective.
The particle update rule is

$$
u^{(i)} \leftarrow u^{(i)} + \varepsilon \phi^\star(u^{(i)}),
$$

where the optimal descent direction is given by

$$
\phi^\star(u)
=
\frac{1}{N} \sum_{j=1}^N
\left[
k(u^{(j)}, u) \nabla_{u^{(j)}} \log p^\star(u^{(j)})
+
\nabla_{u^{(j)}} k(u^{(j)}, u)
\right].
$$

The first term attracts particles toward regions of low cost,
while the second term acts as a repulsive force that prevents particle collapse.
As a result, SV-MPPI is capable of maintaining multimodal control distributions.

However, this kernelized interaction incurs a computational cost of $O(N^2)$ per update.

---

## 5. Gaussian-Restricted Reverse KL and Ensemble Kalman Inversion

We now restrict the variational family to Gaussians

$$
q(u) = \mathcal{N}(\mu, \Sigma).
$$

The optimization problem becomes

$$
\min_{\mu, \Sigma}
\mathrm{KL}\left( \mathcal{N}(\mu, \Sigma) \,\|\, p^\star(u) \right).
$$

Rather than differentiating $J(u)$ directly, we reinterpret the problem as an inverse problem

$$
y = G(u) + \eta,
$$

where

$$
y = 0, \quad G(u) = J(u), \quad \eta \sim \mathcal{N}(0, \lambda).
$$

Applying Ensemble Kalman Inversion yields the ensemble update

$$
u^{(i)} \leftarrow u^{(i)}
-
C_{uJ}
\left(C_{JJ} + \lambda \right)^{-1}
\left(J(u^{(i)}) - \bar{J}\right),
$$

where the empirical covariances are defined as

$$
C_{uJ}
=
\frac{1}{N} \sum_{i=1}^N
(u^{(i)} - \bar{u})(J(u^{(i)}) - \bar{J}),
$$

$$
C_{JJ}
=
\frac{1}{N} \sum_{i=1}^N
(J(u^{(i)}) - \bar{J})^2.
$$

This update can be interpreted as a Gauss–Newton or natural-gradient step
on the manifold of Gaussian distributions.

---

## 6. Mode Collapse and Safety Properties of EKI

Unlike SVGD, EKI does not include an explicit repulsive term.
All ensemble members are updated along directions determined by empirical
control–cost correlations.

As a result, multimodal distributions collapse to a single dominant mode.
Importantly, this collapse is **data-consistent**:
if no ensemble member passes through an obstacle, the mean update cannot move into it.

This explains why EKI-based updates often produce smooth and safe control actions
despite aggressive mode selection.

---

## 7. Comparison Between SVGD and EKI

Both methods minimize the same reverse KL objective, but under different approximations.

| Method | Variational family | Repulsion | Complexity | Behavior |
|------|-------------------|-----------|------------|----------|
| SV-MPPI | Particle-based | Yes | $O(N^2)$ | Multimodal |
| EKI-MPPI | Gaussian | No | $O(N)$ | Mode-selective |

From this perspective, EKI can be viewed as a low-rank, kernel-free approximation
to Stein variational inference.

---

## 8. Implications for Receding-Horizon Control

In receding-horizon control, the planner is invoked at every time step.
Policy refinement therefore occurs naturally over time.

In this setting, a **single EKI update per planning step** is often sufficient
and more stable than iterating EKI to convergence.
This makes EKI particularly attractive for real-time local planning.

---

## 9. Summary

- MPPI can be interpreted as approximate inference over a control posterior.
- SV-MPPI performs nonparametric reverse KL minimization using Stein variational updates.
- Replacing the MPPI update with EKI corresponds to Gaussian-restricted reverse KL minimization.
- EKI induces principled mode collapse without producing infeasible averaged controls.
- For local planning, EKI provides a computationally efficient alternative to SVGD-based methods.

---

## References

- Williams et al., *Model Predictive Path Integral Control*
- Liu and Wang, *Stein Variational Gradient Descent*
- Huang et al., *Stein Variational MPPI*
- Iglesias et al., *Ensemble Kalman Inversion*
