---
title: "[Research Note] 딥러닝, 최적화를 넘어 추론으로: NTK와 GaBP의 만남"
date: 2025-12-21 16:29:00 +0900
categories: [AI Research, Bayesian Methods, Distributed AI]
tags: 
  - AI Theory
  - NTK(NeuralTangentKernel)
  - Distributed Computing
  - Gaussian Belief Propagation
  - Optimization
author: Geonwoo Kim
permalink: /posts/2025/12/blog-post-2/
---

<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$', '$$'], ['\\[', '\\]']] 
    },
    svg: {
      fontCache: 'global'
    }
  };
</script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
</script>

## Introduction: NTK and GaBP

**딥러닝(Deep Learning)**은 현재 AI의 주류이지만, 여전히 수백만 개의 파라미터를 가진 '블랙박스'입니다. 우리는 왜 이 모델이 잘 작동하는지, 결과가 얼마나 확실한지 수학적으로 완벽하게 설명하기 어렵습니다.

하지만 최근 이론적 연구들은 **"신경망의 너비가 무한해지면(Infinite Width), 신경망은 가우시안 프로세스(Gaussian Process)와 동일하다"**는 것을 증명해냈습니다. 이 글에서는 이 흥미로운 이론(NTK)을 바탕으로, 기존의 경사 하강법(SGD) 기반 최적화가 아닌 **Gaussian Belief Propagation(GaBP)**을 이용한 새로운 분산형 추론 방법론을 제안하고자 합니다.

---

## 1. 무한한 신경망과 가우시안 프로세스 (NN-GP Correspondence)

우리가 흔히 사용하는 신경망은 유한한 개수의 뉴런을 가집니다. 하지만 사고 실험을 통해 은닉층(Hidden Layer)의 뉴런 수를 **무한대($N \to \infty$)**로 늘려보면 어떻게 될까요?

### 중심극한정리 (Central Limit Theorem)

신경망의 출력 $f(x)$는 초기화된 가중치(Weight)와 활성화 함수를 통과한 수많은 뉴런들의 합으로 표현됩니다. 통계학의 **중심극한정리**에 따르면, 독립적인 확률변수(뉴런)들의 합이 무한히 많아지면 그 분포는 **가우시안 분포(정규분포)**를 따르게 됩니다.

![Neural Network Width Limit Visualization](/images/post\2025-12-21-blog-post-2/Infinitely_wide_neural_network.webm)
*(그림 1: 신경망 너비가 증가함에 따라 출력이 가우시안 분포로 수렴하는 과정)*

즉, 무한 너비의 신경망은 파라미터 하나하나를 따지는 복잡한 모델이 아니라, 입력 $x$에 대해 출력 $f(x)$가 평균과 공분산(커널)을 가지는 **가우시안 프로세스(Gaussian Process, GP)**로 수학적으로 정의됩니다.

> **핵심:** 무한 신경망은 복잡한 블랙박스가 아니라, **커널 함수(Kernel Function)로 정의되는 명확한 확률 모델**입니다.

---

## 2. 함수 공간(Function-Space)에서의 해석: 최적화에서 선형 대수로

신경망을 GP로 해석하게 되면, 딥러닝의 학습 과정은 패러다임이 완전히 바뀝니다.

* **기존 관점 (Weight-Space):** 손실 함수(Loss)를 줄이기 위해 수백만 개의 가중치 $w$를 미분하고 조금씩 수정(Backpropagation)하며 산을 내려간다. (비볼록 최적화)
* **NTK 관점 (Function-Space):** 데이터 간의 관계를 나타내는 **커널 행렬(Neural Tangent Kernel)**을 계산하고, 이를 이용해 정답 함수 $f(x)$를 한 번에 찾아낸다. (볼록 최적화)

<div style="text-align: center; margin: 20px 0;">
  <img src="/images/post/2025-12-21-blog-post-2/image2.png" alt="Neural Tangent Kernel Matrix Concept" width="80%" style="border-radius: 10px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
  <br>
  <em style="color: gray; font-size: 0.9em;">이때 학습(Fitting)은 복잡한 반복 과정 없이, 아래와 같은 **단순한 선형 방정식(Linear System)**을 푸는 문제로 귀결됩니다.</em>
</div>


$$
(K(X, X) + \sigma^2 I) \mathbf{\alpha} = \mathbf{y}
$$

여기서 $K$는 NTK 커널 행렬, $\mathbf{y}$는 정답 레이블입니다. 이 식을 풀면 신경망의 예측값은 물론, 예측의 **불확실성(Uncertainty)**까지 수학적으로 유도할 수 있습니다.

---

## 3. 문제점: 계산 복잡도의 장벽

이론적으로는 완벽해 보이지만, 치명적인 단점이 있습니다. 바로 **계산 비용**입니다.

데이터가 $N$개일 때, $N \times N$ 크기의 커널 행렬 $K$를 만들고 역행렬(Inverse)을 구해야 하는데, 이 연산의 복잡도는 $O(N^3)$입니다.

* 데이터가 10만 개만 되어도 슈퍼컴퓨터로 계산이 불가능합니다.
* 데이터가 분산되어 있는 환경(IoT, 병원, 엣지 디바이스)에서는 중앙 집중식 행렬 연산 자체가 불가능합니다.

그렇다면, **"거대한 행렬을 뒤집지 않고, 네트워크 상에서 효율적으로 이 방정식을 풀 수는 없을까?"**

---

## 4. 제안하는 해법: GaBP (Gaussian Belief Propagation)

본 연구 노트에서는 이 문제를 해결하기 위해 **Gaussian Belief Propagation (GaBP)** 알고리즘을 제안합니다.

### 아이디어: 데이터를 노드로, 커널을 네트워크로

거대한 행렬 연산을 수행하는 대신, 신경망의 추론 과정을 **확률 그래픽 모델(Probabilistic Graphical Model)**로 변환합니다.

1.  **그래프 구성:** 각 데이터 포인트(또는 연산 장치)를 그래프의 **노드**로 봅니다.
2.  **연결 (Edge):** 데이터 간의 유사도(커널 값, NTK)가 높은 노드끼리 연결합니다.
3.  **메시지 전파 (Message Passing):** 각 노드는 자신의 데이터와 이웃의 정보를 바탕으로 확률 메시지(평균, 분산)를 주고받습니다.

![Gaussian Belief Propagation Graph](/images/post/2025-12-21-blog-post-2/image3.png)
*(그림 2: 데이터 노드 간의 메시지 전파를 통한 분산 추론 구조)*

### GaBP를 통한 분산 최적화

GaBP는 수학적으로 $Ax=b$와 같은 선형 시스템을 푸는 **반복적 솔버(Iterative Solver)** 역할을 수행할 수 있습니다. 이를 NTK에 적용하면 다음과 같은 이점을 얻습니다.

* **분산 처리 (Distributed Inference):** 중앙 서버 없이도 각 노드가 국소적인 통신만으로 전체 데이터에 최적화된(Global Optimum) 해를 찾아갑니다.
* **불확실성 추정 (Uncertainty Awareness):** GaBP는 값(평균)뿐만 아니라 신뢰도(분산)를 함께 전파합니다. 따라서 결과가 **"얼마나 확실한지"**를 알 수 있습니다.
* **효율성:** 희소화(Sparsification) 기법과 결합하면 $O(N^3)$의 벽을 넘어 선형에 가까운 시간 복잡도로 대규모 NTK 추론이 가능해집니다.

---

## 5. 결론 및 기대 효과

본 제안은 **"무한 신경망 이론(NTK)"**과 **"확률적 메시지 전파(GaBP)"**를 결합하여, 딥러닝을 해석 가능한 분산 시스템으로 재구성하는 시도입니다.

이 방법론이 적용된다면 다음과 같은 미래가 가능합니다.

1.  **설명 가능한 AI (XAI):** 결과와 함께 불확실성을 제공하여 신뢰할 수 있는 AI를 구현할 수 있습니다.
2.  **Serverless AI:** 중앙 집중식 학습 없이, 수많은 엣지 디바이스가 협력하여 거대한 지능을 형성하는 분산 학습이 가능해집니다.
3.  **데이터 프라이버시:** 원본 데이터를 공유하지 않고 확률 정보만 교환하므로 보안성이 뛰어납니다.

